---
phase: 02-profiling-engine
plan: 03
type: execute
wave: 3
depends_on: ["02-01", "02-02"]
files_modified:
  - get-shit-done/bin/gsd-tools.js
  - get-shit-done/bin/gsd-tools.test.js
autonomous: true

must_haves:
  truths:
    - "Running `gsd-tools.js profile-questionnaire` outputs a structured JSON analysis matching the same schema as the profiler agent output, with all 8 dimensions populated"
    - "Each questionnaire question includes scenario framing context before presenting options, not just bare multiple-choice"
    - "All questionnaire-derived dimensions have confidence MEDIUM for strong picks or LOW for 'it varies' selections -- never HIGH (because self-report, not observed behavior)"
    - "Tests verify profile-sample produces JSONL with project-proportional sampling"
    - "Tests verify write-profile renders analysis JSON into USER-PROFILE.md with sensitive content filtering"
    - "Tests verify profile-questionnaire produces valid analysis JSON matching the profiler schema"
  artifacts:
    - path: "get-shit-done/bin/gsd-tools.js"
      provides: "profile-questionnaire subcommand"
      contains: "cmdProfileQuestionnaire"
    - path: "get-shit-done/bin/gsd-tools.test.js"
      provides: "Phase 2 test suites for profile-sample, write-profile, profile-questionnaire"
      contains: "profile-sample"
  key_links:
    - from: "main() switch"
      to: "cmdProfileQuestionnaire()"
      via: "case 'profile-questionnaire' dispatch"
      pattern: "case 'profile-questionnaire'"
    - from: "cmdProfileQuestionnaire()"
      to: "PROFILING_QUESTIONS constant"
      via: "question definitions array"
      pattern: "PROFILING_QUESTIONS"
    - from: "profile-questionnaire output"
      to: "write-profile input"
      via: "same JSON schema compatibility"
      pattern: "profile_version"
---

<objective>
Build the questionnaire fallback that produces the same profile structure as session analysis, and add comprehensive tests for all Phase 2 gsd-tools functionality (profile-sample, write-profile, profile-questionnaire).

Purpose: The questionnaire is the fallback path for developers who opt out of session analysis or have no sessions (PROF-05). It must produce the exact same JSON schema so downstream consumers (write-profile, CLAUDE.md generation, /dev-preferences) work identically regardless of data source. Tests ensure all three subcommands work correctly in isolation and produce compatible output.

Output: `gsd-tools.js` extended with `profile-questionnaire` subcommand, `gsd-tools.test.js` extended with Phase 2 test suites.
</objective>

<execution_context>
@/Users/canodevelopment/.claude/get-shit-done/workflows/execute-plan.md
@/Users/canodevelopment/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-profiling-engine/02-RESEARCH.md
@.planning/phases/02-profiling-engine/02-CONTEXT.md
@.planning/phases/02-profiling-engine/02-01-SUMMARY.md
@.planning/phases/02-profiling-engine/02-02-SUMMARY.md
@get-shit-done/bin/gsd-tools.js
@get-shit-done/bin/gsd-tools.test.js
@get-shit-done/references/user-profiling.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement profile-questionnaire subcommand with scenario-framed questions</name>
  <files>get-shit-done/bin/gsd-tools.js</files>
  <action>
Add the `PROFILING_QUESTIONS` constant and `cmdProfileQuestionnaire()` to gsd-tools.js under the Session Pipeline Commands section (after `cmdWriteProfile`), and wire into main switch.

**PROFILING_QUESTIONS constant:**

Define an array of 8 question objects, one per dimension. Each question uses the thoughtful scenario-framing style per user decision. Place this constant near the top of the Session Pipeline Commands section (above the command functions) so it is clearly a data definition.

Per user decision: Each question includes context framing ("Think about the last few times you...") before presenting options.

```javascript
const PROFILING_QUESTIONS = [
  {
    dimension: 'communication_style',
    header: 'Communication Style',
    context: 'Think about the last few times you asked Claude to build or change something. How did you frame the request?',
    question: 'When you ask Claude to build something, how much context do you typically provide?',
    options: [
      { label: 'Minimal -- "fix the bug", "add dark mode", just say what\'s needed', value: 'a', rating: 'terse-direct' },
      { label: 'Some context -- explain what and why in a paragraph or two', value: 'b', rating: 'conversational' },
      { label: 'Detailed specs -- headers, numbered lists, problem analysis, constraints', value: 'c', rating: 'detailed-structured' },
      { label: 'It depends on the task -- simple tasks get short prompts, complex ones get detailed specs', value: 'd', rating: 'mixed' },
    ],
  },
  {
    dimension: 'decision_speed',
    header: 'Decision Making',
    context: 'Think about times when Claude presented you with multiple options -- like choosing a library, picking an architecture, or selecting an approach.',
    question: 'When Claude presents you with options, how do you typically decide?',
    options: [
      { label: 'Pick quickly based on gut feeling or past experience', value: 'a', rating: 'fast-intuitive' },
      { label: 'Ask for a comparison table or pros/cons, then decide', value: 'b', rating: 'deliberate-informed' },
      { label: 'Research independently (read docs, check GitHub stars) before deciding', value: 'c', rating: 'research-first' },
      { label: 'Let Claude recommend -- I generally trust the suggestion', value: 'd', rating: 'delegator' },
    ],
  },
  {
    dimension: 'explanation_depth',
    header: 'Explanation Preferences',
    context: 'Think about when Claude explains code it wrote or an approach it took. How much detail feels right?',
    question: 'When Claude explains something, how much detail do you want?',
    options: [
      { label: 'Just the code -- I\'ll read it and figure it out myself', value: 'a', rating: 'code-only' },
      { label: 'Brief explanation with the code -- a sentence or two about the approach', value: 'b', rating: 'concise' },
      { label: 'Detailed walkthrough -- explain the approach, trade-offs, and code structure', value: 'c', rating: 'detailed' },
      { label: 'Deep dive -- teach me the concepts behind it so I understand the fundamentals', value: 'd', rating: 'educational' },
    ],
  },
  {
    dimension: 'debugging_approach',
    header: 'Debugging Style',
    context: 'Think about the last few times something broke in your code. How did you approach it with Claude?',
    question: 'When something breaks, how do you typically approach debugging with Claude?',
    options: [
      { label: 'Paste the error and say "fix it" -- get it working fast', value: 'a', rating: 'fix-first' },
      { label: 'Share the error plus context, ask Claude to diagnose what went wrong', value: 'b', rating: 'diagnostic' },
      { label: 'Investigate myself first, then ask Claude about my specific theories', value: 'c', rating: 'hypothesis-driven' },
      { label: 'Walk through the code together step by step to understand the issue', value: 'd', rating: 'collaborative' },
    ],
  },
  {
    dimension: 'ux_philosophy',
    header: 'UX Philosophy',
    context: 'Think about user-facing features you have built recently. How did you balance functionality with design?',
    question: 'When building user-facing features, what do you prioritize?',
    options: [
      { label: 'Get it working first, polish the UI later (or never)', value: 'a', rating: 'function-first' },
      { label: 'Basic usability from the start -- nothing ugly, but no pixel-perfection', value: 'b', rating: 'pragmatic' },
      { label: 'Design and UX are as important as functionality -- I care about the experience', value: 'c', rating: 'design-conscious' },
      { label: 'I mostly build backend, CLI, or infrastructure -- UX is minimal', value: 'd', rating: 'backend-focused' },
    ],
  },
  {
    dimension: 'vendor_philosophy',
    header: 'Library & Vendor Choices',
    context: 'Think about the last time you needed a library or service for a project. How did you go about choosing it?',
    question: 'When choosing libraries or services, what is your typical approach?',
    options: [
      { label: 'Use whatever Claude suggests -- speed matters more than the perfect choice', value: 'a', rating: 'pragmatic-fast' },
      { label: 'Prefer well-known, battle-tested options (React, PostgreSQL, Express)', value: 'b', rating: 'conservative' },
      { label: 'Research alternatives, read docs, compare benchmarks before committing', value: 'c', rating: 'thorough-evaluator' },
      { label: 'Strong opinions -- I already know what I like and I stick with it', value: 'd', rating: 'opinionated' },
    ],
  },
  {
    dimension: 'frustration_triggers',
    header: 'Frustration Triggers',
    context: 'Think about moments when working with AI coding assistants that made you frustrated or annoyed.',
    question: 'What frustrates you most when working with AI coding assistants?',
    options: [
      { label: 'Doing things I didn\'t ask for -- adding features, refactoring code, scope creep', value: 'a', rating: 'scope-creep' },
      { label: 'Not following instructions precisely -- ignoring constraints or requirements I stated', value: 'b', rating: 'instruction-adherence' },
      { label: 'Over-explaining or being too verbose -- just give me the code and move on', value: 'c', rating: 'verbosity' },
      { label: 'Breaking working code while fixing something else -- regressions', value: 'd', rating: 'regression' },
    ],
  },
  {
    dimension: 'learning_style',
    header: 'Learning Preferences',
    context: 'Think about encountering something new -- an unfamiliar library, a codebase you inherited, a concept you hadn\'t used before.',
    question: 'When you encounter something new in your codebase, how do you prefer to learn about it?',
    options: [
      { label: 'Read the code directly -- I figure things out by reading and experimenting', value: 'a', rating: 'self-directed' },
      { label: 'Ask Claude to explain the relevant parts to me', value: 'b', rating: 'guided' },
      { label: 'Read official docs and tutorials first, then try things', value: 'c', rating: 'documentation-first' },
      { label: 'See a working example, then modify it to understand how it works', value: 'd', rating: 'example-driven' },
    ],
  },
];
```

**`function cmdProfileQuestionnaire(options, raw)`**

Parameters:
- `options`: `{ answers: string }` -- optional pre-provided answers string for non-interactive mode (e.g., `--answers "a,b,c,d,a,b,c,d"` for testing)
- `raw`: boolean

Logic:
1. If `options.answers` is provided (non-interactive/testing mode):
   - Parse comma-separated answer values (e.g., "a,b,c,d,a,b,c,d")
   - Map each answer to its corresponding dimension
   - This enables testing without interactive prompts

2. If no pre-provided answers (interactive mode):
   - Output each question's `context` framing and `question` text along with the option labels
   - The orchestrating workflow (Phase 3) will use AskUserQuestion to present each question
   - In non-orchestrated mode, output the questions as JSON for the caller to present:
     ```json
     {
       "mode": "interactive",
       "questions": [{ "dimension": "...", "header": "...", "context": "...", "question": "...", "options": [...] }]
     }
     ```
   - When `--answers` is NOT provided, output the questions JSON and exit. The caller collects answers and calls back with `--answers`.

3. When answers are provided, build the analysis JSON matching the profiler agent's output schema:
   ```javascript
   const analysis = {
     profile_version: '1.0',
     analyzed_at: new Date().toISOString(),
     data_source: 'questionnaire',
     projects_analyzed: [],
     messages_analyzed: 0,
     message_threshold: 'questionnaire',
     sensitive_excluded: [],
     dimensions: {},
   };
   ```

4. For each answer:
   - Find the matching question by index
   - Find the selected option by value
   - Determine confidence: if the selected option is the "it varies/depends" style answer (the last option for communication_style, or any option that maps to a "mixed" or context-dependent rating), use LOW. Otherwise use MEDIUM.
   - Per user decision: questionnaire confidence is MEDIUM for strong definitive picks, LOW for ambiguous selections. Never HIGH.
   - Build dimension entry:
     ```javascript
     {
       rating: option.rating,
       confidence: isAmbiguous ? 'LOW' : 'MEDIUM',
       evidence_count: 1,
       cross_project_consistent: null,  // not applicable for questionnaire
       evidence_quotes: [{
         signal: 'Self-reported via questionnaire',
         quote: option.label,
         project: 'N/A (questionnaire)'
       }],
       summary: `Developer self-reported as ${option.rating} for ${question.header.toLowerCase()}.`,
       claude_instruction: generateClaudeInstruction(question.dimension, option.rating),
     }
     ```

5. The `generateClaudeInstruction()` helper maps dimension + rating to an imperative Claude directive. Define a lookup object:
   ```javascript
   const CLAUDE_INSTRUCTIONS = {
     communication_style: {
       'terse-direct': 'Keep responses concise and action-oriented. Skip lengthy preambles. Match this developer\'s direct style.',
       'conversational': 'Use a natural conversational tone. Explain reasoning briefly alongside code. Engage with the developer\'s questions.',
       'detailed-structured': 'Match this developer\'s structured communication: use headers for sections, numbered lists for steps, and acknowledge provided context before responding.',
       'mixed': 'Adapt response detail to match the complexity of each request. Brief for simple tasks, detailed for complex ones.',
     },
     // ... define for all 8 dimensions x all ratings
   };
   ```
   Define CLAUDE_INSTRUCTIONS for ALL dimension/rating combinations. Each instruction is an imperative directive written for Claude's consumption, not a description of the developer.

6. Output the analysis JSON:
   ```javascript
   output(analysis, raw);
   ```

**Main dispatch:**
```javascript
case 'profile-questionnaire': {
  const answersIdx = args.indexOf('--answers');
  const answers = answersIdx !== -1 ? args[answersIdx + 1] : null;
  cmdProfileQuestionnaire({ answers }, raw);
  break;
}
```

Place after the `write-profile` case.
  </action>
  <verify>
```bash
# Verify questionnaire outputs questions when no answers provided
node get-shit-done/bin/gsd-tools.js profile-questionnaire --raw 2>/dev/null | node -e "
const data = JSON.parse(require('fs').readFileSync('/dev/stdin','utf-8'));
console.log(data.mode === 'interactive' ? 'PASS: Interactive mode' : 'FAIL');
console.log(data.questions.length === 8 ? 'PASS: 8 questions' : 'FAIL: ' + data.questions.length + ' questions');
"

# Verify answers mode produces valid analysis JSON
node get-shit-done/bin/gsd-tools.js profile-questionnaire --answers "a,b,c,d,a,b,c,d" --raw 2>/dev/null | node -e "
const data = JSON.parse(require('fs').readFileSync('/dev/stdin','utf-8'));
console.log(data.profile_version === '1.0' ? 'PASS: Has profile_version' : 'FAIL');
console.log(data.data_source === 'questionnaire' ? 'PASS: Correct data_source' : 'FAIL');
console.log(Object.keys(data.dimensions).length === 8 ? 'PASS: 8 dimensions' : 'FAIL');
const confs = Object.values(data.dimensions).map(d => d.confidence);
console.log(confs.every(c => c === 'MEDIUM' || c === 'LOW') ? 'PASS: No HIGH confidence' : 'FAIL: Has HIGH confidence');
"

# Verify syntax
node -c get-shit-done/bin/gsd-tools.js
```
  </verify>
  <done>
`profile-questionnaire` subcommand exists with 8 scenario-framed questions. Without --answers flag, outputs questions JSON for the caller to present. With --answers flag, produces analysis JSON matching the profiler agent's schema exactly. All questionnaire-derived dimensions have MEDIUM or LOW confidence (never HIGH). Each dimension has a claude_instruction imperative directive. Schema is compatible with write-profile's input.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add comprehensive tests for all Phase 2 gsd-tools subcommands</name>
  <files>get-shit-done/bin/gsd-tools.test.js</files>
  <action>
Add Phase 2 test suites to `gsd-tools.test.js`, following the established test patterns from Phase 1 (temp directories with mock data, --path flag for testability, process spawn for CLI testing).

Add three new `describe()` blocks:

**1. `describe('profile-sample', () => { ... })`**

Tests (5-6 tests):
- `it('produces JSONL output with project-proportional sampling')` -- Create a temp sessions directory with 3 mock projects (project-a with 5 sessions, project-b with 2 sessions, project-c with 1 session), each containing mock JSONL with genuine user messages. Run `profile-sample --path <temp> --limit 10 --json`. Verify: output has `output_file`, `projects_sampled` >= 2, `messages_sampled` <= 10, the JSONL file exists and each line is valid JSON with `projectName` field.
- `it('caps messages per project')` -- Create temp with one project having 20 sessions and another with 1 session. Run with `--limit 10`. Verify: no single project contributes all 10 messages (per-project cap enforced).
- `it('enriches messages with projectName field')` -- Run against temp directory. Verify each message in JSONL has a `projectName` string field.
- `it('truncates message content to maxChars')` -- Create a session with a message containing 2000 chars. Run with `--max-chars 100`. Verify output message content length <= 100.
- `it('errors on nonexistent path')` -- Run with `--path /nonexistent`. Verify exit code 1 and error message.

Mock JSONL format (matching Phase 1 convention from test suite):
```javascript
const mockMessage = JSON.stringify({
  type: 'user',
  message: { role: 'user', content: 'fix the bug in the auth flow' },
  userType: 'external',
  cwd: '/mock/project',
  timestamp: new Date().toISOString(),
});
```

**2. `describe('write-profile', () => { ... })`**

Tests (4-5 tests):
- `it('renders analysis JSON into USER-PROFILE.md')` -- Create a temp analysis JSON file with valid profiler output (all 8 dimensions populated). Run `write-profile --input <temp-json> --output <temp-output>`. Verify: output file exists, contains "Developer Profile", contains dimension headers, contains evidence entries.
- `it('applies sensitive content filter')` -- Create analysis JSON with an evidence quote containing "sk-test123456789012345678". Run write-profile. Verify: output file contains "[REDACTED]" instead of the key. Verify stderr contains "Sensitive content redacted".
- `it('creates output directory if missing')` -- Run with `--output <temp>/nested/deep/USER-PROFILE.md`. Verify: file is created successfully (directory auto-created).
- `it('errors when input file missing')` -- Run with `--input /nonexistent.json`. Verify: exit code 1 and error message.
- `it('handles questionnaire-sourced analysis identically')` -- Create analysis JSON with `data_source: "questionnaire"`. Run write-profile. Verify: output file structure is identical to session-analysis-sourced profile.

Mock analysis JSON:
```javascript
const mockAnalysis = {
  profile_version: '1.0',
  analyzed_at: new Date().toISOString(),
  data_source: 'session_analysis',
  projects_analyzed: ['TestProject'],
  messages_analyzed: 50,
  message_threshold: 'full',
  sensitive_excluded: [],
  dimensions: {
    communication_style: {
      rating: 'detailed-structured',
      confidence: 'HIGH',
      evidence_count: 15,
      cross_project_consistent: true,
      evidence_quotes: [
        { signal: 'Uses markdown headers', quote: '## Context\\nThe auth flow...', project: 'TestProject' }
      ],
      summary: 'Consistently provides structured context.',
      claude_instruction: 'Match structured communication with headers and lists.'
    },
    // ... fill all 8 dimensions with test data
  }
};
```

**3. `describe('profile-questionnaire', () => { ... })`**

Tests (4-5 tests):
- `it('outputs questions in interactive mode when no answers provided')` -- Run without --answers. Verify: output has `mode: "interactive"`, `questions` array with 8 entries, each has `dimension`, `context`, `question`, `options`.
- `it('produces valid analysis JSON from answers')` -- Run with `--answers "a,b,c,d,a,b,c,d"`. Verify: output has `profile_version: "1.0"`, `data_source: "questionnaire"`, 8 dimensions, each with rating, confidence, claude_instruction.
- `it('assigns MEDIUM confidence for definitive answers')` -- Run with `--answers "a,a,a,a,a,a,a,a"`. Verify: all dimensions have confidence MEDIUM (none of the "a" options are ambiguous).
- `it('assigns LOW confidence for ambiguous "it varies" answers')` -- Run with `--answers "d,d,d,d,d,d,d,d"`. Verify: communication_style has LOW confidence (the "d" option is the "it depends" choice). Other dimensions may vary based on whether their "d" option is ambiguous.
- `it('produces schema compatible with write-profile')` -- Run questionnaire with answers, pipe output to a temp file, then run write-profile with that as input. Verify: write-profile succeeds and produces valid USER-PROFILE.md.

**Test utilities:**
Use the established pattern from Phase 1 tests:
- `child_process.execSync()` or `child_process.spawnSync()` for CLI invocation
- `fs.mkdtempSync()` for isolated temp directories
- Cleanup in `afterEach()` or `afterAll()`
- Assert with existing test framework assertions (check what Phase 1 uses -- likely Node's built-in assert or a test runner like the one already in the project)
  </action>
  <verify>
```bash
# Run the full test suite
cd /Users/canodevelopment/coding-portfolio/get-shit-done && node --test get-shit-done/bin/gsd-tools.test.js 2>&1 | tail -20
```
Verify all new Phase 2 tests pass alongside existing Phase 1 tests. Expected: 13-16 new tests + 89 existing = 102-105 total tests passing.
  </verify>
  <done>
Test file extended with 3 new describe blocks covering profile-sample (5-6 tests), write-profile (4-5 tests), and profile-questionnaire (4-5 tests). All new tests pass. All existing tests still pass. Total test count: 100+ tests. Tests verify schema compatibility between questionnaire and session analysis paths. Tests verify sensitive content filtering. Tests use temp directories with mock data for isolation.
  </done>
</task>

</tasks>

<verification>
Run these checks to verify Plan 02-03 deliverables:

1. **Syntax check:** `node -c get-shit-done/bin/gsd-tools.js` passes
2. **All tests pass:** `cd /Users/canodevelopment/coding-portfolio/get-shit-done && node --test get-shit-done/bin/gsd-tools.test.js` -- all pass
3. **Questionnaire interactive mode:** `node get-shit-done/bin/gsd-tools.js profile-questionnaire --raw` outputs questions JSON
4. **Questionnaire answers mode:** `node get-shit-done/bin/gsd-tools.js profile-questionnaire --answers "a,b,c,d,a,b,c,d" --raw` outputs analysis JSON
5. **Schema compatibility:** Questionnaire output can be piped to write-profile without errors
6. **No HIGH confidence from questionnaire:** All questionnaire dimensions have MEDIUM or LOW confidence
</verification>

<success_criteria>
- `profile-questionnaire` outputs 8 scenario-framed questions in interactive mode
- `profile-questionnaire --answers` produces analysis JSON with same schema as profiler agent
- Questionnaire confidence is MEDIUM for definitive picks, LOW for ambiguous -- never HIGH
- Each dimension has an imperative claude_instruction directive
- 13-16 new tests covering profile-sample, write-profile, and profile-questionnaire
- All new and existing tests pass
- Questionnaire output is compatible as write-profile input (schema parity verified by test)
</success_criteria>

<output>
After completion, create `.planning/phases/02-profiling-engine/02-03-SUMMARY.md`
</output>
